= Day-2 Operations: Keeping the Factory Running
:navtitle: Operations & Troubleshooting
:toc: macro
:role: SRE & Platform Operations
:audience: Site Reliability Engineers, Platform Admins
:description: How to monitor, debug, and tune the Red Hat OpenShift AI 3.x MaaS stack when things go wrong.

[.lead]
*Building the factory is Step 1. Keeping it running is the rest of your career.*

In a distributed MaaS environment, a "broken" service isn't just a crashing pod. It could be a misconfigured Gateway route, an aggressive rate limit, or a saturated GPU.

This guide covers the critical observability signals and the three most common failure scenarios in the RHOAI 3.x stack.

== 1. The Dashboard: What to Watch

Before debugging, you need visibility. Do not rely on user complaints.

=== Key Metrics (Prometheus / Console)

[cols="1,2,2", options="header"]
|===
| Metric | What it tells you | Alert Threshold

| **TTFT (Time To First Token)**
| The latency user perceives. High TTFT means the system is queuing requests or the GPU is overloaded.
| > 200ms (for interactive chat)

| **Token Generation Rate**
| The throughput of the factory. Sudden drops indicate upstream issues or model crashes.
| < 10 tokens/sec (per stream)

| **Gateway 4xx/5xx Rate**
| **429:** Kuadrant is blocking traffic (Good/Policy). +
**404:** Route misconfiguration (Bad). +
**503:** vLLM backend is down or scaling from zero (Bad).
| > 1% (excluding valid 429s)

| **GPU Memory Utilization**
| How full the KV Cache is. If this hits 100%, vLLM may swap to CPU or reject requests.
| > 95% sustained
|===

== 2. Troubleshooting Scenario A: "The Broken Door" (404 Not Found)

**Symptom:** You `curl` the endpoint and get `404 Not Found`.
**The Component:** Gateway API.

In RHOAI 3.x, the Service Mesh/Gateway is strict. A 404 usually means the **HTTPRoute** is not correctly attached to the **Gateway**.

* **Diagnosis:**
[source,bash]
----
# 1. Check if the HTTPRoute is accepted
oc describe httproute granite-route -n maas-production
----
* **Look for:** `Conditions: Accepted = False`.
* **Common Fixes:**
** **Hostname Mismatch:** Does the `curl` header (`-H "Host: api.maas.example.com"`) match the `hostnames` in the YAML exactly?
** **Gateway Namespace:** Did you reference the Gateway in the `istio-system` (or correct) namespace?
** **Port Mismatch:** Does the `backendRef` point to port `8080` (vLLM default)?

== 3. Troubleshooting Scenario B: "The Stop Sign" (429 Too Many Requests)

**Symptom:** Users report their requests are being rejected.
**The Component:** Kuadrant (Rate Limiting).

This is usually "working as designed," but you need to verify *why*.

* **Diagnosis:**
[source,bash]
----
# 1. Check the RateLimitPolicy status
oc describe ratelimitpolicy granite-protection -n maas-production
----
* **Check the Headers:** Run `curl -v`. Kuadrant injects headers telling you the limit.
** `x-ratelimit-remaining: 0`
** `x-ratelimit-reset: 5`
* **Fix:**
** If legitimate traffic is blocked, edit the `RateLimitPolicy` YAML to increase the `limits` bucket.
** **Pro Tip:** In RHOAI 3.x, ensure you are counting **Tokens**, not just Requests, if you configured specific descriptors.

== 4. Troubleshooting Scenario C: "The Silent Queue" (High Latency / Pending Pods)

**Symptom:** Requests succeed (200 OK) but take 10+ seconds to start (High TTFT).
**The Component:** KEDA & vLLM.

This means the "Queue" is full, and KEDA isn't scaling fast enough.

* **Diagnosis:**
[source,bash]
----
# 1. Check the HorizontalPodAutoscaler (HPA) generated by KEDA
oc get hpa -n maas-production
----
* **Look for:** `TARGETs` showing `10/1` (Current/Target). If the current value is high, KEDA is trying to scale.
* **Why isn't it scaling?**
[source,bash]
----
oc get pods -n maas-production
# Status: Pending?
----
* **Root Cause:** **GPU Starvation.** You have hit the physical limits of your cluster. HPA wants 5 pods, but you only have GPUs for 2.
* **Fix:**
** **Short Term:** Reduce `minReplicas` or `maxReplicas`.
** **Long Term:** Purchase more nodes or enable **MIG (Multi-Instance GPU)** to slice existing cards into smaller units.

== 5. Tuning vLLM for Performance

If your architecture is correct but the model is just *slow*, tune the engine.

[source,yaml]
----
# Inside your ServingRuntime YAML
args:
  - "--gpu-memory-utilization=0.95" # Increase KV Cache space
  - "--max-model-len=4096"          # Cap context to save memory
  - "--enforce-eager"               # Use for debugging CUDA graph issues
----

[TIP]
.The "OOM" Trap
====
If vLLM crashes with `OOMKilled`, it is not usually the *model weights*; it is the **KV Cache** growing too large during long conversations. Decrease `--gpu-memory-utilization` to reserve a buffer for overhead.
====

== Conclusion: You Are Ready

You have completed the **Red Hat OpenShift AI 3.x MaaS Learning Journey**.

* You understand the **Strategy** (MaaS vs. Dedicated).
* You speak the **Taxonomy** (vLLM, Gateway, Kuadrant).
* You have **Built the Factory** (Lab).
* You can **Fix the Machinery** (Troubleshooting).

**Next Steps:**
Go to the **Red Hat Hybrid Cloud Console** and start your pilot.

[.text-center]
**[End of Course]**