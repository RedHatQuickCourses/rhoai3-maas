= Lab: Build Your MaaS Factory
:navtitle: Hands-On Lab
:toc: macro
:role: Developer & Platform Engineer
:audience: Implementation Teams
:description: A step-by-step guide to deploying a model with vLLM, exposing it via Kubernetes Gateway API, and enforcing rate limits with Kuadrant.

[.lead]
*Theory is over. It is time to build.*

In this lab, you will act as the Platform Engineer. You will not just "run a container." You will build a governed manufacturing line.

You will deploy a Lightweight LLM (Granite-7B or similar), expose it through the new **Gateway API**, and apply a **Token-Aware Rate Limit** to prevent cost overruns.

[IMPORTANT]
.Prerequisites
====
* **OpenShift Container Platform 4.16+**
* **Red Hat OpenShift AI Operator** (Channel `fast-3.x`) installed.
* **Red Hat OpenShift Service Mesh** (for Gateway API implementation) installed.
* **Kuadrant Operator** installed (for Rate Limiting).
* Access to a namespace with **GPU quota** (at least 1 NVIDIA A10G/A100 or equivalent).
* The `oc` CLI tool logged in.
====

== Step 1: Initialize the Workspace

First, create a project for your MaaS utility.

[source,bash]
----
oc new-project maas-production
----

== Step 2: Deploy the Inference Engine (vLLM)

We will define a `ServingRuntime` that tells OpenShift AI how to run the vLLM engine, and an `InferenceService` to deploy the specific model.

. **Create the ServingRuntime (vllm-runtime.yaml):**
+
[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  namespace: maas-production
spec:
  supportedModelFormats:
    - name: vLLM
      version: "1"
      autoSelect: true
  multiModel: false
  containers:
    - name: kserve-container
      image: quay.io/modh/vllm:stable
      command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--model=/mnt/models"
        - "--port=8080"
        - "--gpu-memory-utilization=0.9"
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
----

. **Deploy the Model (granite-service.yaml):**
+
[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-7b
  namespace: maas-production
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      storage:
        key: aws-connection-secret # Assuming you have configured S3 storage
        path: models/granite-7b-lab
----

[source,bash]
----
oc apply -f vllm-runtime.yaml
oc apply -f granite-service.yaml
----

*Checkpoint:* Run `oc get pods` and wait until the `granite-7b-predictor` pod is **Running**.

== Step 3: Configure the Front Door (Gateway API)

In RHOAI 3.x, we use the Gateway API to route traffic. This separates the "Infrastructure" from the "Model."

. **Create the HTTPRoute (model-route.yaml):**
+
[source,yaml]
----
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: granite-route
  namespace: maas-production
spec:
  parentRefs:
    - name: maas-gateway
      namespace: istio-system # or where your Gateway is defined
  hostnames:
    - "api.maas.example.com"
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/completions
      backendRefs:
        - name: granite-7b-predictor
          port: 8080
----

[source,bash]
----
oc apply -f model-route.yaml
----

== Step 4: Apply Governance (Kuadrant)

Now, we solve the "Why Buy" challenge. We will apply a policy that limits users to **5 requests every 10 seconds** to prevent abuse.

. **Create the RateLimitPolicy (safety-limit.yaml):**
+
[source,yaml]
----
apiVersion: kuadrant.io/v1beta2
kind: RateLimitPolicy
metadata:
  name: granite-protection
  namespace: maas-production
spec:
  targetRef:
    group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: granite-route
  limits:
    "basic-tier":
      rates:
        - limit: 5
          window: 10s
----

[source,bash]
----
oc apply -f safety-limit.yaml
----

== Step 5: Verification & Stress Test

Let's prove the factory works and the guards are active.

. **Test 1: A Valid Request (The "Happy Path")**
+
[source,bash]
----
curl -X POST "http://api.maas.example.com/v1/completions" \
     -H "Content-Type: application/json" \
     -d '{"model": "granite-7b", "prompt": "Define industrialized AI.", "max_tokens": 50}'
----
+
*Result:* You should receive a JSON response with the definition.

. **Test 2: The Stress Test (The "Governance Path")**
+
Run this loop to simulate a "noisy neighbor" or a runaway script:
+
[source,bash]
----
for i in {1..20}; do
  curl -o /dev/null -s -w "%{http_code}\n" -X POST "http://api.maas.example.com/v1/completions" \
  -H "Content-Type: application/json" \
  -d '{"model": "granite-7b", "prompt": "spam", "max_tokens": 10}'
done
----

*Result:*
You will see `200` (Success) for the first 5 requests.
Then, you will see `429` (Too Many Requests) for the rest.

**Congratulations.** You have successfully deployed a MaaS architecture that is:
1.  **Efficient** (Shared vLLM Runtime).
2.  **Accessible** (Standard Gateway API).
3.  **Governed** (Kuadrant Rate Limiting).

[.text-center]
*You have built the factory. Now, keep it running.* +
link:troubleshooting.html[**Next: Day-2 Operations & Troubleshooting >**]

```