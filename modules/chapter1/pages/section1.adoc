= Strategy & Taxonomy: Defining Your Path to Production
:navtitle: Strategy & Taxonomy
:toc: macro
:role: Architect & Technical Lead
:audience: Cloud Architects, ML Engineers, Platform Engineers
:description: Define the strategic deployment models in Red Hat OpenShift AI 3.x and master the essential vocabulary of the new MaaS stack.

[.lead]
Before rushing to deploy a container, you must choose your architectural strategy.

In Red Hat OpenShift AI 3.x, "model serving" has evolved into two distinct paths. Choosing the wrong path leads to either wasted infrastructure spend or unmanageable operational complexity.

This section defines those paths and establishes the vocabulary necessary to navigate the new RHOAI 3.x landscape.

== The Strategic Divide: Dedicated vs. Utility

Not every AI workload is the same. Serving a massive, fine-tuned model for a single, high-frequency trading application requires a different architecture than providing general-purpose LLM access to 500 enterprise developers.

We categorize these approaches into two paths: **Dedicated Serving** and **Models as a Service (MaaS) Utility**.

[cols="1,3,3", options="header"]
|===
| Feature | Path A: Dedicated Serving (The "Specialist") | Path B: MaaS Utility (The "Generalist")

| **Analogy**
| A private race car. Optimized for one driver and maximum speed on a specific track.
| A city bus system. Optimized for reliability, sharing resources efficiently among many riders, and metered usage.

| **Use Case**
| A single, critical application needing guaranteed, exclusive throughput. E.g., A fine-tuned 70B parameter model used solely by a fraud detection system.
| Providing standardized API access (e.g., `gpt-3.5-turbo` equivalent) to multiple internal teams for varied tasks like summarization, coding assistance, or RAG.

| **Infrastructure Focus**
| **Performance Isolation.** The model claims entire GPUs. No sharing.
| **Cost Efficiency & Governance.** GPUs are sliced using MIG or shared via time-slicing to maximize utilization.

| **Key Technologies**
| KServe with a single-node runtime (e.g., vLLM dedicated instance or NVIDIA NIM).
| **The MaaS Stack:** Distributed vLLM, Gateway API for routing, and Kuadrant for multi-tenant rate limiting.

| **Primary Pain Point**
| Low utilization. If the app isn't using the GPU, it sits idle.
| "Noisy neighbors." Ensuring one team's heavy usage doesn't degrade performance for others.
|===

[IMPORTANT]
.Crucial RHOAI 3.x Change: TGIS is Removed
====
In previous versions of OpenShift AI, the Text Generation Inference Server (TGIS) was a common runtime. **TGIS has been removed in RHOAI 3.x.**

The new standard for high-performance inference across both dedicated and MaaS paths is **vLLM**, renowned for its PagedAttention algorithm that dramatically improves throughput.
====

**This course focuses exclusively on Path B: The MaaS Utility.** We are building the factory that serves many, not the garage that builds one race car.

== The New Vocabulary: RHOAI 3.x Taxonomy

To build a MaaS utility, you must master a new set of components that replace older Kubernetes paradigms.

[NOTE]
.The Shift from Routes to Gateways
====
The most significant architectural shift in RHOAI 3.x MaaS is moving away from standard OpenShift `Routes` for model traffic. We now use the Kubernetes **Gateway API**. This separates the role of the *Infrastructure Administrator* (who manages listeners, IPs, and SSL) from the *ML Engineer* (who just wants to attach a route to their service).
====

Here are the essential terms for the MaaS stack:

vLLM (The Engine):::
The default high-performance serving runtime in RHOAI 3.x. It uses PagedAttention to manage memory efficiently, allowing for higher batch sizes and faster token generation. In a MaaS setup, it is often deployed in a distributed manner.

Gateway API (The Doorman):::
The new Kubernetes standard for networking. It handles ingress traffic. In MaaS, a centralized "Gateway" accepts all API calls (e.g., `api.example.com/v1/completions`) and routes them to the correct backend model service based on the path or header.

Kuadrant (The Bouncer & Meter):::
The governance layer. It integrates with the Gateway API to enforce policies. Crucially for LLMs, Kuadrant provides **token-aware rate limiting**. It doesn't just count HTTP requests; it counts the actual cost (input + output tokens) of a request to prevent budget overruns by a single tenant.

TrustyAI (The Watchdog):::
The observability and compliance layer. It monitors model inputs and outputs over time to detect **data drift** (is live data different from training data?) and **bias** (is the model treating different groups unfairly?).

== Next Steps: Choose Your Path

You understand the strategy and you speak the language. Now, look at the blueprint.

[.text-center]
*Where do you want to go next?* +
+
--
link:architecture.html[**Option 1: Deep Dive into the Architecture Stack**] +
*(Recommended for Architects & Platform Engineers needing the technical "how".)*
--
+
--
link:lab-setup.html[**Option 2: Skip straight to the Hands-On Lab**] +
*(Recommended for Developers who want to deploy "Hello World" immediately.)*
--