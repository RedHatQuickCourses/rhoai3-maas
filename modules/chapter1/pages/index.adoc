= Industrializing AI: The Business Case for Models as a Service
:navtitle: Introduction & Business Value
:toc: macro
:role: Executive & Architectural Strategy
:audience: CTOs, Enterprise Architects, Platform Leads
:description: Why enterprises are moving from ticket-based AI to industrialized Models as a Service (MaaS) on Red Hat OpenShift AI 3.x to solve ROI, security, and scale challenges.

[.lead]
*Stop treating AI like a science project. Start running it like a factory.*

In the early days of enterprise AI, data scientists were explorers. They requested hardware, downloaded models to their laptops, and ran experiments in isolation.

Today, that model is broken.

As demand for Generative AI explodes, the "ticket-based" workflow—where teams file IT tickets to reserve GPUs—has become a bottleneck. Expensive H100s sit idle at night while teams fight for capacity during the day. "Shadow AI" proliferates as frustrated developers bypass IT to use public APIs, leaking proprietary data.

**Models as a Service (MaaS)** on Red Hat OpenShift AI 3.x is the answer. It is the shift from manual, static deployments to a **centralized, API-first utility** that optimizes cost, enforces security, and guarantees performance.



== The Core Objection: "Why not just use a Public API?"

This is the first question every CTO asks: _"Why build this? Why not just point our developers to OpenAI or Anthropic?"_

Public APIs are excellent for prototyping, but they fail at enterprise scale for three reasons:

. **The Cost Explosion:** Public APIs charge per token. As you scale to thousands of employees and automated agents, costs become unpredictable and linear. MaaS allows you to **cap your costs** by owning the compute.
. **The Privacy Gap:** Sending sensitive IP, PII, or financial data to a public endpoint violates Zero Trust principles. With MaaS, traffic never leaves your cluster.
. **The "Noisy Neighbor" Latency:** Public APIs offer no SLA on latency. In a mission-critical app, a 3-second pause is a failure. MaaS gives you predictable **Time-To-First-Token (TTFT)**.

== The 3 Pillars of MaaS Value

Red Hat OpenShift AI 3.x introduces specific features to solve these challenges.

=== 1. Economic Efficiency (The "Idle Iron" Problem)
* **The Problem:** A single model rarely utilizes 100% of an A100 GPU 100% of the time. If you dedicate one GPU per team, you are paying for idle silicon.
* **The MaaS Solution:** **Dynamic GPU Slicing & Sharing.**
Using technologies like **MIG (Multi-Instance GPU)** and the **vLLM** runtime, MaaS allows you to partition a single physical GPU into multiple "slices." You can run 4 distinct small models on one card, or serve 50 different teams from a shared pool, driving your hardware utilization from 20% to 80%.

=== 2. Governance at the Edge (The "Wild West" Problem)
* **The Problem:** Once a model is deployed, who is using it? How much? Are they authorized?
* **The MaaS Solution:** **Token-Aware Rate Limiting.**
RHOAI 3.x integrates **Kuadrant** and the **Gateway API**. Unlike standard load balancers that only count "requests," Kuadrant counts **tokens**. You can create strict policies (e.g., "The Marketing Team gets 1M tokens/day; Engineering gets 5M"). This prevents a single runaway script from draining your budget or crashing the system.

=== 3. Developer Velocity (The "Friction" Problem)
* **The Problem:** Developers don't want to learn Kubernetes, KServe, or Pod specs. They just want an endpoint.
* **The MaaS Solution:** **Standardized APIs.**
The platform exposes an **OpenAI-compatible schema**. Developers simply change the `base_url` in their code. They don't know (or care) if the model is running on an NVIDIA H100 or an Intel Gaudi accelerator; they just get a response.

== Your Learning Journey

This course transforms you from a "Cluster Admin" into an "AI Platform Architect."

[cols="1,3", options="header"]
|===
| Module | What You Will Learn

| **1. Strategy (The Choice)**
| Stop guessing. Learn when to use **Dedicated Serving** (for massive, single models) vs. **MaaS/Distributed Serving** (for shared, high-concurrency APIs).

| **2. Taxonomy (The Language)**
| Master the RHOAI 3.x vocabulary: **vLLM** (The Engine), **Gateway API** (The Router), and **TrustyAI** (The Watchdog).

| **3. Architecture (The Blueprint)**
| A deep dive into the stack: How **KEDA** scales based on queue depth and how **Authorino** enforces Zero Trust.

| **4. Lab (The Build)**
| **Hands-on:** You will deploy the OpenShift AI Operator, configure the **Gateway**, deploy a model with **vLLM**, and apply a **Rate Limit Policy** to block abusive traffic.
|===

[.text-center]
*Ready to build the factory?* +
link:strategy.html[**Step 1: Choose Your Strategy (Dedicated vs. MaaS) >**]

```