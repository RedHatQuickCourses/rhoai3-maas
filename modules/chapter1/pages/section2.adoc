= The MaaS Stack: Architecture Deep Dive
:navtitle: Architecture Deep Dive
:toc: macro
:role: Architect & Platform Engineer
:audience: Platform Leads, SREs, ML Ops
:description: A technical breakdown of the OpenShift AI 3.x MaaS stack, including vLLM, Gateway API, KEDA, and TrustyAI.

[.lead]
You have chosen the MaaS utility path. Now, let's look at the blueprint.

To run AI as an industrialized service, you need more than just a container. You need a stack that handles **inference**, **routing**, **governance**, and **observability**.

This section breaks down the "MaaS Stack" layer by layer, showing how Red Hat OpenShift AI 3.x orchestrates these components to deliver a secure, scalable API.

image::maas-architecture-stack.png[Diagram showing the 4 layers: Hardware -> Inference (vLLM) -> Networking (Gateway) -> Governance (Kuadrant/TrustyAI)]

== Layer 1: The Inference Engine (vLLM)

At the heart of the stack is **vLLM**, the high-performance serving runtime.

* **Role:** It loads the model weights and executes the neural network forward pass.
* **Key Feature:** **PagedAttention**. Unlike legacy runtimes that fragment memory, vLLM manages GPU memory like an operating system manages RAM (using pages). This allows for higher batch sizes and throughput.
* **Deployment:** vLLM runs as a standard Pod, but in a MaaS configuration, it is often deployed via the **Service Mesh** or **Serverless** components to support scale-to-zero.

== Layer 2: Role-Oriented Networking (Gateway API)

In RHOAI 3.x, we move away from simple OpenShift Routes to the **Kubernetes Gateway API**. This splits the responsibility of "exposing the service" into two distinct roles:

. **The Infrastructure Architect** manages the **Gateway**.
* They define the "Front Door" (IP address, TLS certificates, load balancer hardware).
* *Manifest:* `Gateway`

. **The ML Engineer** manages the **HTTPRoute**.
* They simply "attach" their model to the Gateway.
* *Manifest:* `HTTPRoute` (e.g., "Route traffic from `api.corp.com/v1/models/llama-3` to my vLLM service").

[NOTE]
.Why this matters
====
This separation means developers can deploy new models without needing cluster-admin privileges to configure ingress controllers. It is the technical enabler of "Self-Service."
====

== Layer 3: The Governance Plane (Kuadrant)

An industrialized factory needs a foreman to ensure safety and efficiency. **Kuadrant** is that foreman.

It connects to the Gateway API to enforce policies **before** traffic hits the expensive GPUs.

* **Rate Limiting:** Protects the system from being overwhelmed by a single user.
* **Token-Awareness:** This is critical for LLMs. A request with 10 tokens costs less than a request with 10,000 tokens. Kuadrant can rate-limit based on the *aggregate token count*, ensuring true cost control.
* **AuthN/AuthZ:** Integrates with **Authorino** to enforce OIDC authentication (Keycloak, Entra ID) at the edge.

== Layer 4: Observability & Trust (TrustyAI)

The final layer ensures the factory is producing quality goods. **TrustyAI** monitors the model's behavior in real-time.

* **Bias Detection:** Is the model showing statistical bias against protected groups in its output?
* **Drift Detection:** Is the incoming data significantly different from the training data? (e.g., users asking about 2024 events when the model was trained on 2022 data).
* **Explainability:** Providing insights into *why* a model gave a specific response.

== Scaling Logic: KEDA

How do we handle a burst of traffic? We don't just look at CPU usage.

**KEDA (Kubernetes Event-Driven Autoscaling)** scales the vLLM pods based on "application intelligence."

* **The Old Way (HPA):** Scale when CPU > 80%. (Too slow for AI; the queue is already full).
* **The MaaS Way (KEDA):** Scale when `serving_queue_depth > 5`.
* **Result:** The system spins up new pods *before* latency degrades, ensuring a consistent **Time-To-First-Token (TTFT)**.

== Summary: The Request Lifecycle

1. **User** sends a curl request to `api.corp.com`.
2. **Gateway** receives the traffic.
3. **Kuadrant** checks: "Does this user have quota remaining?" (If no -> 429 Error).
4. **KEDA** checks: "Is the queue too deep?" (If yes -> Spin up more pods).
5. **vLLM** processes the request using PagedAttention.
6. **TrustyAI** asynchronously logs the input/output for bias analysis.
7. **Response** is streamed back to the user.

[.text-center]
*You have the blueprints. Now, let's build the factory.* +
link:lab-setup.html[**Next: Hands-On Lab - Deploying Your First MaaS Endpoint >**]